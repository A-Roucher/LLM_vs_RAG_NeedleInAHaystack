{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: faiss-gpu in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: load_dotenv in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from load_dotenv) (1.0.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Requirement already satisfied: langchain in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.0.341)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.0.7,>=0.0.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (0.0.6)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (0.0.67)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (1.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (3.1.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: openai in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.3.5)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (2.4.2)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install faiss-gpu\n",
    "!pip install load_dotenv\n",
    "!pip install tiktoken\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "from scripts import generate_context, retrieve_relevant_excerpts\n",
    "from embeddings import retrieve_relevant_excerpts_quickly\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "needle_question_couples = [\n",
    "    (\"\\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\", \"What is the most fun thing to do in San Francisco?\"),\n",
    "    (\"\\nThe most inspiring thing to do near the Hugging Face office in Paris is to visit the Louvre museum.\\n\", \"What is the most inspiring thing to do near the Hugging Face office in Paris?\"),\n",
    "]\n",
    "\n",
    "needle, question = needle_question_couples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Test retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    encode_kwargs={'normalize_embeddings': False},\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from scripts import result_exists, evaluate_response\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The code will check to see if a context_length, depth percent and version number have already been checked yet\n",
    "# Change the version # if you would like to run the results multiple times.\n",
    "# If you're just testing, then leave as version=1\n",
    "results_version = 1 \n",
    "\n",
    "# This will produce a list of context lengths for each experiment iteration. Make sure the max context length is within the bounds of your models limits.\n",
    "context_lengths = np.round(np.linspace(1000, 128000, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "# This will product a list of document depths to place your random statement (needle) at.\n",
    "# Suggestion: Try out different distributions (like a sigmoid) to test non-evenly space intervals\n",
    "document_depth_percents = np.round(np.linspace(0, 100, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "evaluation_model  = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.inference_api import InferenceApi\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "client = InferenceApi(\n",
    "    repo_id=model_id,\n",
    "    token=os.getenv('HUGGINGFACEHUB_API_TOKEN', 'YourHuggingFaceToken'),\n",
    ")\n",
    "\n",
    "# This will get logged on your results\n",
    "model_to_test_description = 'embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI bot that answers questions for a user. Keep your response short and direct.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"\n",
    "        You will have to answer this question based only on the context: {question}\n",
    "        Here is the context: {context}\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"\n",
    "        Answer the question: {question}\n",
    "        Don't give information outside the document or repeat your findings.\n",
    "        \"\"\"\n",
    "    }\n",
    " ]\n",
    "\n",
    "messages_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "full_prompt = messages_chat.format(question=question, context=context[:1000])\n",
    "output = client(full_prompt)\n",
    "\n",
    "output[0]['generated_text'][len(full_prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through each iteration of context_lengths and depths\n",
    "for depth_percent in tqdm(document_depth_percents):\n",
    "    for context_length in context_lengths:\n",
    "        # Load results from file. \n",
    "        try:\n",
    "            with open(f'output/results_{model_to_test_description}.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = []\n",
    "            pass\n",
    "\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if result_exists(results, context_length, depth_percent, results_version, model_to_test_description):\n",
    "            continue\n",
    "\n",
    "        # Go generate the required length context and place your needle statement in\n",
    "        context = generate_context(needle, context_length, depth_percent)\n",
    "\n",
    "        context = await retrieve_relevant_excerpts_quickly(context, question, hf_embedding)\n",
    "\n",
    "        # Go see if the model can answer the question to pull out your random fact\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI bot that answers questions for a user. Keep your response short and direct.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "            You will have to answer this question based only on the context: {question}\n",
    "            Here is the context: {context}\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "            Answer the question: {question}\n",
    "            Don't give information outside the document or repeat your findings.\n",
    "            \"\"\"}\n",
    "        ]\n",
    "\n",
    "        messages_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        full_prompt = messages_chat.format(question=question, context=context)\n",
    "        assert len(tokenizer.encode(full_prompt)) < 4096, \"Your prompt is too long. Try a shorter context length or a smaller document depth.\"\n",
    "        output = client(full_prompt)\n",
    "\n",
    "        response = output[0]['generated_text'][len(full_prompt):]\n",
    "\n",
    "        # Compare the reponse to the actual needle you placed\n",
    "        score = evaluate_response(response, needle, question, evaluation_model)\n",
    "\n",
    "        results.append({\n",
    "            # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "            'model' : model_to_test_description,\n",
    "            'context_length' : int(context_length),\n",
    "            'depth_percent' : int(depth_percent),\n",
    "            'version' : results_version,\n",
    "            'needle' : needle,\n",
    "            'model_response' : response,\n",
    "            'score' : score\n",
    "        })\n",
    "\n",
    "        print (f\"Result #: {len(results)}/{len(context_lengths) * len(document_depth_percents)}\")\n",
    "        print (f\"Context: {context_length} tokens\")\n",
    "        print (f\"Depth: {depth_percent}%\")\n",
    "        print (f\"Score: {score}\")\n",
    "        print (f\"Response: {response}\\n\")\n",
    "\n",
    "        # Save results to a JSON file each run\n",
    "        with open(f'output/results_{model_to_test_description}.json', 'w') as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_test = ChatOpenAI(model='gpt-4', temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))\n",
    "\n",
    "model_to_test_description = 'gpt_rag'\n",
    "\n",
    "# Run through each iteration of context_lengths and depths\n",
    "for depth_percent in tqdm(document_depth_percents):\n",
    "    for context_length in context_lengths:\n",
    "        # Load results from file. \n",
    "        try:\n",
    "            with open(f'output/results_{model_to_test_description}.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = []\n",
    "            pass\n",
    "\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if result_exists(results, context_length, depth_percent, results_version, model_to_test_description):\n",
    "            continue\n",
    "\n",
    "        # Go generate the required length context and place your needle statement in\n",
    "        context = generate_context(needle, context_length, depth_percent)\n",
    "\n",
    "        context = await retrieve_relevant_excerpts_quickly(context, question, hf_embedding)\n",
    "\n",
    "        # Prepare your message to send to the model you're going to evaluate\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful AI bot that answers questions for a user. Keep your response short and direct\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the PG essays with your needle/random statement placed in it\n",
    "                # This is your haystack with a needle placed in it.\n",
    "                content=f\"CONTEXT:\\n{context}\",\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the question you'll ask to the model to tr≠≠y and retrieve your random statement/needle.\n",
    "                content=f\"{question} - Don't give information outside the document or repeat your findings\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # Go see if the model can answer the question to pull out your random fact\n",
    "        response = model_to_test(messages)\n",
    "\n",
    "        # Compare the reponse to the actual needle you placed\n",
    "        score = evaluate_response(response, needle, question, evaluation_model)\n",
    "\n",
    "        results.append({\n",
    "            # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "            'model' : model_to_test_description,\n",
    "            'context_length' : int(context_length),\n",
    "            'depth_percent' : int(depth_percent),\n",
    "            'version' : results_version,\n",
    "            'needle' : needle,\n",
    "            'model_response' : response.content,\n",
    "            'score' : score\n",
    "        })\n",
    "\n",
    "        print (f\"Result #: {len(results)}/{len(context_lengths) * len(document_depth_percents)}\")\n",
    "        print (f\"Context: {context_length} tokens\")\n",
    "        print (f\"Depth: {depth_percent}%\")\n",
    "        print (f\"Score: {score}\")\n",
    "        print (f\"Response: {response.content}\\n\")\n",
    "\n",
    "        # Save results to a JSON file each run\n",
    "        with open(f'output/results_{model_to_test_description}.json', 'w') as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Long Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code will check to see if a context_length, depth percent and version number have already been checked yet\n",
    "# Change the version # if you would like to run the results multiple times.\n",
    "# If you're just testing, then leave as version=1\n",
    "results_version = 1 \n",
    "\n",
    "# This will produce a list of context lengths for each experiment iteration. Make sure the max context length is within the bounds of your models limits.\n",
    "context_lengths = np.round(np.linspace(1000, 128000, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "# This will product a list of document depths to place your random statement (needle) at.\n",
    "# Suggestion: Try out different distributions (like a sigmoid) to test non-evenly space intervals\n",
    "document_depth_percents = np.round(np.linspace(0, 100, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "# The model we are testing. As of now it's set up for chat models with OpenAI\n",
    "model_to_test = ChatOpenAI(model='gpt-4-1106-preview', temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))\n",
    "\n",
    "\n",
    "# This will get logged on your results\n",
    "model_to_test_description = 'gpt4'\n",
    "\n",
    "evaluation_model  = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))\n",
    "\n",
    "# Run through each iteration of context_lengths and depths\n",
    "for context_length in context_lengths:\n",
    "    for depth_percent in document_depth_percents:\n",
    "        # Load results from file. \n",
    "        try:\n",
    "            with open('output/results.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = []\n",
    "            pass\n",
    "\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if result_exists(results, context_length, depth_percent, results_version, model_to_test_description):\n",
    "            continue\n",
    "\n",
    "        # Go generate the required length context and place your needle statement in\n",
    "        context = generate_context(needle, context_length, depth_percent)\n",
    "\n",
    "        # Prepare your message to send to the model you're going to evaluate\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful AI bot that answers questions for a user. Keep your response short and direct\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the PG essays with your needle/random statement placed in it\n",
    "                # This is your haystack with a needle placed in it.\n",
    "                content=context\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the question you'll ask to the model to tr≠≠y and retrieve your random statement/needle.\n",
    "                content=\"What is the most fun thing to do in San Francico based on the context? Don't give information outside the document or repeat your findings\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # Go see if the model can answer the question to pull out your random fact\n",
    "        response = model_to_test(messages)\n",
    "\n",
    "        # Compare the reponse to the actual needle you placed\n",
    "        score = evaluate_response(response, needle, question, evaluation_model)\n",
    "\n",
    "        results.append({\n",
    "            # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "            'model' : model_to_test_description,\n",
    "            'context_length' : int(context_length),\n",
    "            'depth_percent' : int(depth_percent),\n",
    "            'version' : results_version,\n",
    "            'needle' : needle,\n",
    "            'model_response' : response.content,\n",
    "            'score' : score\n",
    "        })\n",
    "\n",
    "        print (f\"Result #: {len(results)}/{len(context_lengths) * len(document_depth_percents)}\")\n",
    "        print (f\"Context: {context_length} tokens\")\n",
    "        print (f\"Depth: {depth_percent}%\")\n",
    "        print (f\"Score: {score}\")\n",
    "        print (f\"Response: {response.content}\\n\")\n",
    "\n",
    "        # Save results to a JSON file each run\n",
    "        with open('outpout/results.json', 'w') as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "        # Optional. Sleep for a bit to stay under the rate limit\n",
    "        # Rate limit is 150K tokens/min so it's set at 120K for some cushion\n",
    "        sleep_time = (context_length / 120000)*60\n",
    "        # print (f\"Sleeping: {sleep_time}\\n\")\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with open('output/results_gpt_rag.json', 'r') as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "table_rag = pd.DataFrame(results)\n",
    "table_rag = table_rag.pivot_table(index='depth_percent', columns='context_length', values='score') / 10\n",
    "mask = (table_rag >= 0.3)\n",
    "table_rag = table_rag.where(mask, 0)\n",
    "\n",
    "table_long_context = pd.read_csv('original_results/gpt4.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "\n",
    "def display_table(table):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(table)\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(table.columns)), labels=table.columns)\n",
    "    ax.set_yticks(np.arange(len(table.index)), labels=table.index)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(table.index)):\n",
    "        for j in range(len(table.columns)):\n",
    "            text = ax.text(j, i, table.values[i, j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_table(table_rag)\n",
    "display_table(table_long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
