{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install faiss-gpu\n",
    "!pip install load_dotenv\n",
    "!pip install tiktoken\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install openai\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "from scripts import generate_context, retrieve_relevant_excerpts\n",
    "from embeddings import retrieve_relevant_excerpts_quickly\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needle_question_couples = [\n",
    "    (\"\\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\", \"What is the most fun thing to do in San Francisco?\"),\n",
    "    (\"\\nThe most inspiring thing to do near the Hugging Face office in Paris is to visit the Louvre museum.\\n\", \"What is the most inspiring thing to do near the Hugging Face office in Paris?\"),\n",
    "]\n",
    "\n",
    "needle, question = needle_question_couples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Test retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = generate_context(needle, 100000, 40)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    encode_kwargs={'normalize_embeddings': False},\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = retrieve_relevant_excerpts(context, question, hf_embedding)\n",
    "print(len(documents))\n",
    "print(documents[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = await retrieve_relevant_excerpts_quickly(context, question, hf_embedding)\n",
    "print(len(documents))\n",
    "print(documents[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from scripts import result_exists, evaluate_response\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The code will check to see if a context_length, depth percent and version number have already been checked yet\n",
    "# Change the version # if you would like to run the results multiple times.\n",
    "# If you're just testing, then leave as version=1\n",
    "results_version = 1 \n",
    "\n",
    "# This will produce a list of context lengths for each experiment iteration. Make sure the max context length is within the bounds of your models limits.\n",
    "context_lengths = np.round(np.linspace(1000, 128000, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "# This will product a list of document depths to place your random statement (needle) at.\n",
    "# Suggestion: Try out different distributions (like a sigmoid) to test non-evenly space intervals\n",
    "document_depth_percents = np.round(np.linspace(0, 100, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "evaluation_model  = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.inference_api import InferenceApi\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "client = InferenceApi(\n",
    "    repo_id=model_id,\n",
    "    token=os.getenv('HUGGINGFACEHUB_API_TOKEN', 'YourHuggingFaceToken'),\n",
    ")\n",
    "\n",
    "# This will get logged on your results\n",
    "model_to_test_description = 'embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI bot that answers questions for a user. Keep your response short and direct.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"\n",
    "        You will have to answer this question based only on the context: {question}\n",
    "        Here is the context: {context}\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": \"\"\"\n",
    "        Answer the question: {question}\n",
    "        Don't give information outside the document or repeat your findings.\n",
    "        \"\"\"\n",
    "    }\n",
    " ]\n",
    "\n",
    "messages_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "full_prompt = messages_chat.format(question=question, context=context[:1000])\n",
    "output = client(full_prompt)\n",
    "\n",
    "output[0]['generated_text'][len(full_prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through each iteration of context_lengths and depths\n",
    "for depth_percent in tqdm(document_depth_percents):\n",
    "    for context_length in context_lengths:\n",
    "        # Load results from file. \n",
    "        try:\n",
    "            with open(f'output/results_{model_to_test_description}.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = []\n",
    "            pass\n",
    "\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if result_exists(results, context_length, depth_percent, results_version, model_to_test_description):\n",
    "            continue\n",
    "\n",
    "        # Go generate the required length context and place your needle statement in\n",
    "        context = generate_context(needle, context_length, depth_percent)\n",
    "\n",
    "        context = await retrieve_relevant_excerpts_quickly(context, question, hf_embedding)\n",
    "\n",
    "        # Go see if the model can answer the question to pull out your random fact\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI bot that answers questions for a user. Keep your response short and direct.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "            You will have to answer this question based only on the context: {question}\n",
    "            Here is the context: {context}\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "            Answer the question: {question}\n",
    "            Don't give information outside the document or repeat your findings.\n",
    "            \"\"\"}\n",
    "        ]\n",
    "\n",
    "        messages_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        full_prompt = messages_chat.format(question=question, context=context)\n",
    "        assert len(tokenizer.encode(full_prompt)) < 4096, \"Your prompt is too long. Try a shorter context length or a smaller document depth.\"\n",
    "        output = client(full_prompt)\n",
    "\n",
    "        response = output[0]['generated_text'][len(full_prompt):]\n",
    "\n",
    "        # Compare the reponse to the actual needle you placed\n",
    "        score = evaluate_response(response, needle, question, evaluation_model)\n",
    "\n",
    "        results.append({\n",
    "            # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "            'model' : model_to_test_description,\n",
    "            'context_length' : int(context_length),\n",
    "            'depth_percent' : int(depth_percent),\n",
    "            'version' : results_version,\n",
    "            'needle' : needle,\n",
    "            'model_response' : response,\n",
    "            'score' : score\n",
    "        })\n",
    "\n",
    "        print (f\"Result #: {len(results)}/{len(context_lengths) * len(document_depth_percents)}\")\n",
    "        print (f\"Context: {context_length} tokens\")\n",
    "        print (f\"Depth: {depth_percent}%\")\n",
    "        print (f\"Score: {score}\")\n",
    "        print (f\"Response: {response}\\n\")\n",
    "\n",
    "        # Save results to a JSON file each run\n",
    "        with open(f'output/results_{model_to_test_description}.json', 'w') as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_test = ChatOpenAI(model='gpt-4', temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))\n",
    "\n",
    "model_to_test_description = 'gpt_rag'\n",
    "\n",
    "# Run through each iteration of context_lengths and depths\n",
    "for depth_percent in tqdm(document_depth_percents):\n",
    "    for context_length in context_lengths:\n",
    "        # Load results from file. \n",
    "        try:\n",
    "            with open(f'output/results_{model_to_test_description}.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = []\n",
    "            pass\n",
    "\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if result_exists(results, context_length, depth_percent, results_version, model_to_test_description):\n",
    "            continue\n",
    "\n",
    "        # Go generate the required length context and place your needle statement in\n",
    "        context = generate_context(needle, context_length, depth_percent)\n",
    "\n",
    "        context = await retrieve_relevant_excerpts_quickly(context, question, hf_embedding)\n",
    "\n",
    "        # Prepare your message to send to the model you're going to evaluate\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful AI bot that answers questions for a user. Keep your response short and direct\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the PG essays with your needle/random statement placed in it\n",
    "                # This is your haystack with a needle placed in it.\n",
    "                content=f\"CONTEXT:\\n{context}\",\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the question you'll ask to the model to tr≠≠y and retrieve your random statement/needle.\n",
    "                content=f\"{question} - Don't give information outside the document or repeat your findings\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # Go see if the model can answer the question to pull out your random fact\n",
    "        response = model_to_test(messages)\n",
    "\n",
    "        # Compare the reponse to the actual needle you placed\n",
    "        score = evaluate_response(response, needle, question, evaluation_model)\n",
    "\n",
    "        results.append({\n",
    "            # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "            'model' : model_to_test_description,\n",
    "            'context_length' : int(context_length),\n",
    "            'depth_percent' : int(depth_percent),\n",
    "            'version' : results_version,\n",
    "            'needle' : needle,\n",
    "            'model_response' : response.content,\n",
    "            'score' : score\n",
    "        })\n",
    "\n",
    "        print (f\"Result #: {len(results)}/{len(context_lengths) * len(document_depth_percents)}\")\n",
    "        print (f\"Context: {context_length} tokens\")\n",
    "        print (f\"Depth: {depth_percent}%\")\n",
    "        print (f\"Score: {score}\")\n",
    "        print (f\"Response: {response.content}\\n\")\n",
    "\n",
    "        # Save results to a JSON file each run\n",
    "        with open(f'output/results_{model_to_test_description}.json', 'w') as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Long Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code will check to see if a context_length, depth percent and version number have already been checked yet\n",
    "# Change the version # if you would like to run the results multiple times.\n",
    "# If you're just testing, then leave as version=1\n",
    "results_version = 1 \n",
    "\n",
    "# This will produce a list of context lengths for each experiment iteration. Make sure the max context length is within the bounds of your models limits.\n",
    "context_lengths = np.round(np.linspace(1000, 128000, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "# This will product a list of document depths to place your random statement (needle) at.\n",
    "# Suggestion: Try out different distributions (like a sigmoid) to test non-evenly space intervals\n",
    "document_depth_percents = np.round(np.linspace(0, 100, num=15, endpoint=True)).astype(int)\n",
    "\n",
    "# The model we are testing. As of now it's set up for chat models with OpenAI\n",
    "model_to_test = ChatOpenAI(model='gpt-4-1106-preview', temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))\n",
    "\n",
    "\n",
    "# This will get logged on your results\n",
    "model_to_test_description = 'gpt4'\n",
    "\n",
    "evaluation_model  = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKey'))\n",
    "\n",
    "# Run through each iteration of context_lengths and depths\n",
    "for context_length in context_lengths:\n",
    "    for depth_percent in document_depth_percents:\n",
    "        # Load results from file. \n",
    "        try:\n",
    "            with open('output/results.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = []\n",
    "            pass\n",
    "\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if result_exists(results, context_length, depth_percent, results_version, model_to_test_description):\n",
    "            continue\n",
    "\n",
    "        # Go generate the required length context and place your needle statement in\n",
    "        context = generate_context(needle, context_length, depth_percent)\n",
    "\n",
    "        # Prepare your message to send to the model you're going to evaluate\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful AI bot that answers questions for a user. Keep your response short and direct\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the PG essays with your needle/random statement placed in it\n",
    "                # This is your haystack with a needle placed in it.\n",
    "                content=context\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                # This is the question you'll ask to the model to tr≠≠y and retrieve your random statement/needle.\n",
    "                content=\"What is the most fun thing to do in San Francico based on the context? Don't give information outside the document or repeat your findings\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # Go see if the model can answer the question to pull out your random fact\n",
    "        response = model_to_test(messages)\n",
    "\n",
    "        # Compare the reponse to the actual needle you placed\n",
    "        score = evaluate_response(response, needle, question, evaluation_model)\n",
    "\n",
    "        results.append({\n",
    "            # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "            'model' : model_to_test_description,\n",
    "            'context_length' : int(context_length),\n",
    "            'depth_percent' : int(depth_percent),\n",
    "            'version' : results_version,\n",
    "            'needle' : needle,\n",
    "            'model_response' : response.content,\n",
    "            'score' : score\n",
    "        })\n",
    "\n",
    "        print (f\"Result #: {len(results)}/{len(context_lengths) * len(document_depth_percents)}\")\n",
    "        print (f\"Context: {context_length} tokens\")\n",
    "        print (f\"Depth: {depth_percent}%\")\n",
    "        print (f\"Score: {score}\")\n",
    "        print (f\"Response: {response.content}\\n\")\n",
    "\n",
    "        # Save results to a JSON file each run\n",
    "        with open('outpout/results.json', 'w') as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "        # Optional. Sleep for a bit to stay under the rate limit\n",
    "        # Rate limit is 150K tokens/min so it's set at 120K for some cushion\n",
    "        sleep_time = (context_length / 120000)*60\n",
    "        # print (f\"Sleeping: {sleep_time}\\n\")\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with open('output/results_gpt_rag.json', 'r') as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "table_rag = pd.DataFrame(results)\n",
    "table_rag = table_rag.pivot_table(index='depth_percent', columns='context_length', values='score') / 10\n",
    "mask = (table_rag >= 0.3)\n",
    "table_rag = table_rag.where(mask, 0)\n",
    "\n",
    "table_long_context = pd.read_csv('original_results/gpt4.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "\n",
    "def display_table(table):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(table)\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(table.columns)), labels=table.columns)\n",
    "    ax.set_yticks(np.arange(len(table.index)), labels=table.index)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(table.index)):\n",
    "        for j in range(len(table.columns)):\n",
    "            text = ax.text(j, i, table.values[i, j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_table(table_rag)\n",
    "display_table(table_long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
