{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install faiss-gpu\n",
    "!pip install load_dotenv\n",
    "!pip install tiktoken\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from scripts import generate_context, retrieve_relevant_excerpts\n",
    "from embeddings import retrieve_relevant_excerpts_quickly\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needle_question_couples = [\n",
    "    (\"\\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\", \"What is the most fun thing to do in San Francisco?\"),\n",
    "    (\"\\nThe most inspiring thing to do near the Hugging Face office in Paris is to visit the Louvre museum.\\n\", \"What is the most inspiring thing to do near the Hugging Face office in Paris?\"),\n",
    "]\n",
    "\n",
    "needle, question = needle_question_couples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Test retrieval\n",
    "\n",
    "We test an Information Retrieval pipeline by first inserting a small piece of information (the _needle_) inside a very long text.\n",
    "Thus we use `generate_context` to choose the length of the resulting context, in tokens.\n",
    "- The longer the context, the harder it will be to find this small needle of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = generate_context(needle, context_length=100000, depth_percent=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Context has {len(context)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    encode_kwargs={'normalize_embeddings': False},\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now based on the chosen `embedding`, we will retrieve the most relevant documents from te context to answer the given `question` (related to the `needle`).\n",
    "\n",
    "### Retrieve documents on local machine (vanilla method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retrieve_relevant_excerpts(context, question, embedding)\n",
    "print(len(retrieved_documents))\n",
    "print(retrieved_documents[:300])\n",
    "print(\"(...)\")\n",
    "print(retrieved_documents[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve documents with Text embeddings inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = await retrieve_relevant_excerpts_quickly(context, question, embedding)\n",
    "print(len(retrieved_documents))\n",
    "print(retrieved_documents[:300])\n",
    "print(\"(...)\")\n",
    "print(retrieved_documents[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ‘‰ The computation runs much faster with Text embeddings inference.\n",
    "\n",
    "Why is that?\n",
    "As per the [repo's Readme](https://github.com/huggingface/text-embeddings-inference):\n",
    "\n",
    ">\n",
    "> TEI implements many features such as:\n",
    "> - No model graph compilation step\n",
    "> - Small docker images and fast boot times. Get ready for true serverless!\n",
    "> - Token based dynamic batching\n",
    "> - Optimized transformers code for inference using Flash Attention, Candle and cuBLASLt\n",
    "> - Safetensors weight loading\n",
    "> - Production ready (distributed tracing with Open Telemetry, Prometheus metrics)\n",
    ">\n",
    ">    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
