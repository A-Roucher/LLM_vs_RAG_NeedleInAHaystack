{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output/results_gpt4_rag.json', 'r') as file:\n",
    "#     results = json.load(file)\n",
    "# print(len(results))\n",
    "# results = [el for el in results if int(el['context_length']) != 28214]\n",
    "\n",
    "# with open(f'output/results_gpt4_rag.json', 'w') as f:\n",
    "#     json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(experiment_name: str) -> pd.DataFrame:\n",
    "    with open(f'output/results_{experiment_name}.json', 'r') as file:\n",
    "        results = json.load(file)\n",
    "\n",
    "    result = pd.DataFrame(results)\n",
    "    result['score'] = result['score'].apply(lambda x: (x if x > 3 else 0))\n",
    "    result['score'] = result['score'] / 10\n",
    "    average = result.groupby(\"context_length\")['score'].mean()\n",
    "    average = pd.DataFrame(average).rename(columns={'score': 'average_score'}).T\n",
    "    return result, average\n",
    "\n",
    "result_os_rag, average_os_rag = load_results('Zephyr-7b_RAG')\n",
    "result_gpt_rag, average_gpt_rag = load_results('GPT4_RAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_long_context = pd.read_csv('original_results/gpt4.csv', index_col=0) / 10\n",
    "average_long_context = table_long_context.mean(axis=0)\n",
    "average_long_context = pd.DataFrame(average_long_context).rename(columns={'score': 'average_score'}).T\n",
    "average_long_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = pd.concat([average_long_context, average_os_rag, average_gpt_rag], axis=0).T\n",
    "comb.columns = ['Long Context', 'Mistral7b_RAG', 'GPT_RAG']\n",
    "comb = pd.DataFrame(comb.unstack().reset_index())\n",
    "comb.columns = ['model', 'context_length', 'average_score']\n",
    "comb['context_length'] = comb['context_length'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two figures: one fore the lines, one for the point markers\n",
    "fig1 = px.scatter(comb, color=\"model\", x='context_length', y='average_score', title='Accuracy of Retrieval - RAG vs Long-Context GPT4', width=1000)\n",
    "fig2 = px.line(comb, color=\"model\", x='context_length', y='average_score', title='Accuracy of Retrieval - RAG vs Long-Context GPT4', width=1000).add_traces(fig1.data)\n",
    "\n",
    "fig2.update_layout(yaxis_range=[0.6, 1.05])\n",
    "fig2.update_layout(xaxis=dict(range=[0,130000]), yaxis_tickformat=',.0%', font=dict(\n",
    "        family=\"Arial\",\n",
    "        size=18,\n",
    "        color=\"grey\"\n",
    "    ))\n",
    "\n",
    "\n",
    "fig2.update_layout(\n",
    "    # paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(1,1,1,0.1)',\n",
    ")\n",
    "fig2.update_xaxes(tickvals = list(range(10000, 130000, 10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom colormap. Go to https://coolors.co/ and pick cool colors\n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#F0496E\", \"#EBB839\", \"#0CD79F\"])\n",
    "\n",
    "pivot_table = result_gpt_rag.groupby(['depth_percent', 'context_length'])['score'].mean().unstack()\n",
    "\n",
    "# Create the heatmap with better aesthetics\n",
    "plt.figure(figsize=(17.5, 8))  # Can adjust these dimensions as needed\n",
    "sns.heatmap(\n",
    "    pivot_table,\n",
    "    # annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=cmap,\n",
    "    cbar_kws={'label': 'Score'},\n",
    "    linecolor='grey', linewidth=0.5,\n",
    ")\n",
    "plt.gca().collections[0].set_clim(0,1)\n",
    "\n",
    "# More aesthetics\n",
    "plt.title('Pressure Testing GPT-4 128K Context\\nFact Retrieval Across Context Lengths (\"Needle In A HayStack\")')  # Adds a title\n",
    "plt.xlabel('Token Limit')  # X-axis label\n",
    "plt.ylabel('Depth Percent')  # Y-axis label\n",
    "plt.xticks(rotation=45)  # Rotates the x-axis labels to prevent overlap\n",
    "plt.yticks(rotation=0)  # Ensures the y-axis labels are horizontal\n",
    "plt.tight_layout()  # Fits everything neatly into the figure area\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
